{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "from torcheval.metrics.metric import Metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torcheval.metrics import MulticlassPrecision\n",
    "from torcheval.metrics import MulticlassRecall\n",
    "from torcheval.metrics import MulticlassConfusionMatrix\n",
    "from torcheval.metrics import MulticlassF1Score\n",
    "\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torcheval.metrics import BinaryPrecision\n",
    "from torcheval.metrics import BinaryRecall\n",
    "from torcheval.metrics import BinaryConfusionMatrix\n",
    "from torcheval.metrics import BinaryF1Score\n",
    "\n",
    "from torcheval.metrics.functional import mean_squared_error\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGITS:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "NUMERICAL LABELS:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Logits:\n",
    "#   -> classes\n",
    "# |\n",
    "# v  samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-accuracy: 0.853\n",
      "micro_accuracy: 0.962\n",
      "accuracy_per_class:\n",
      "  Blues: 0.811\n",
      "  Classical: 0.932\n",
      "  Country: 0.743\n",
      "MCC: 0.534\n"
     ]
    }
   ],
   "source": [
    "dict_metric = {\n",
    "    \"macro-accuracy\": 0.853,\n",
    "    \"micro_accuracy\": 0.962,\n",
    "    \"accuracy_per_class\": {\n",
    "        \"Blues\": 0.811,\n",
    "        \"Classical\": 0.932,\n",
    "        \"Country\": 0.743\n",
    "    },\n",
    "    \"MCC\": 0.534\n",
    "}\n",
    "\n",
    "def print_metric_dictionary(metric_dict):\n",
    "    for key, value in metric_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"{key}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"  {sub_key}: {sub_value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "            \n",
    "print_metric_dictionary(dict_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 10\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "metrics:\n",
      "  macro-accuracy: 0.853\n",
      "  micro_accuracy: 0.962\n",
      "  accuracy_per_class: {'Blues': 0.811, 'Classical': 0.932, 'Country': 0.743}\n",
      "  MCC: 0.534\n"
     ]
    }
   ],
   "source": [
    "dict_full = {\"epochs\": 10, \"batch_size\": 32, \"learning_rate\": 0.001}\n",
    "\n",
    "dict_full[\"metrics\"] = dict_metric\n",
    "print_metric_dictionary(dict_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
